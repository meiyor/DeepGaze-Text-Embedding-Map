model:
  downscale_factor: 2
  readout_factor: 4
  saliency_map_factor: 4
  include_previous_x: true
  include_previous_y: true
  included_durations: []
  included_previous_fixations: [-1,-2]
  features:
    features1:
      type: "deepgaze_pytorch.features.vgg.VGG19NamedFeatures"
      used_features:
        - conv5_1
        - relu5_1
        - relu5_2
        - conv5_3
        - relu5_4
  saliency_network:
    input_channels: 3481
    layers:
    - name: conv1
      channels: 16
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
    - name: conv2
      channels: 8
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
    - name: conv3
      channels: 1
      layer_norm: True
      bias: True
      activation_fn: softplus
  saliency_network_TEM:
    input_channels: 303
    layers:
    - name: conv1_n
      channels: 64
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
    - name: conv2_n
      channels: 16
      layer_norm: True
      batch_norm: True
      bias: True
      activation_fn: softplus
  scanpath_network:
    input_channels: 300
    layers:
     - name: conv1
       channels: 128
       layer_norm: True
       batch_norm: True
     - name: conv2
       channels: 16
       layer_norm: True
       batch_norm: True
  fixation_selection_network:
    input_channels: [1, 0] #or use [1 16] in case you will use the scanpath 
    layers:
    - name: conv1
      channels: 64
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
    - name: conv2
      channels: 16
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
    - name: conv3
      channels: 1
      activation_fn: null
      bias: False
  conv_all_parameters:
    input_channels: 21
    layers:
    - name: conv1
      channels: 300
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
  conv_all_parameters_trans:
    input_channels: 21                           
    layers:
    - name: convtrans 
      channels: 300
      layer_norm: True
      batch_norm: True
      activation_fn: softplus
training:
  optimizer:
    type: torch.optim.AdaBound
    params:
      lr: 0.001
  lr_scheduler:
    type: torch.optim.lr_scheduler.MultiStepLR
    params:
     #milestones: [20, 25, 50, 60, 70, 90, 105, 120]
     milestones: [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33]
  batch_size: 10
  iteration_element: image
  averaging_element: image
  training_dataset_ratio_per_epoch: 1.0

  parts:
  - name: pretraining
    model:
      downscale_factor: 1.5
    train_dataset:
      stimuli: pysaliency_datasets/SALICON/stimuli_train.hdf5
      fixations: pysaliency_datasets/SALICON/fixations_train.hdf5
      centerbias: ../SaliencyFeatures/experiments/experiment0000_SALICON_centerbias/SALICON_train_baseline.hdf5
    val_dataset:
      stimuli: pysaliency_datasets/SALICON/stimuli_val.hdf5
      fixations: pysaliency_datasets/SALICON/fixations_val.hdf5
      centerbias: ../SaliencyFeatures/experiments/experiment0007_SALICON_val_centerbias/SALICON_val_baseline.hdf5
    lr_scheduler:
      params:
        milestones:  [28, 32, 40, 60, 70, 90, 105, 120]
    validation_metrics: ['IG','LL', 'AUC', 'NSS']
    validation_metric: LL
    evaluation:
      compute_metrics:
        metrics: ['IG', 'LL', 'AUC', 'NSS']
        datasets: ['training', 'validation']
    #final_cleanup:
    #  cleanup_checkpoints: true
  - name: Baseline
    train_dataset:
        stimuli: pysaliency_datasets/MIT1003_twosize_initial_fixation/stimuli.hdf5
        fixations: pysaliency_datasets/MIT1003_twosize_initial_fixation/fixations.hdf5
        filters:
        - type: filter_fixations_by_number
          parameters:
            intervals:
            - [2, 1000]
    centerbias: experiments/experiment0001_centerbias_from_third_fixation/centerbias.hdf5
    crossvalidation:
      folds: 5
      val_folds: 1
      test_folds: 1
    startwith: "{root_directory}/pretraining_2/final.pth"
    evaluation:
      compute_metrics:
        metrics: ['IG', 'LL', 'AUC', 'NSS']
        datasets: ['training', 'validation', 'test']
      compute_predictions:
        datasets: ['validation', 'test']
